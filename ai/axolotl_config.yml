# ============================================
# Axolotl Fine-Tuning Config for SR&ED Report Writer
# ============================================
# Model: Mistral-7B-Instruct (good balance of quality & speed)
# Task: Generate CRA-compliant T661 SR&ED project descriptions
# ============================================

base_model: mistralai/Mistral-7B-Instruct-v0.3
model_type: MistralForCausalLM
tokenizer_type: LlamaTokenizer
trust_remote_code: true

# Load in 4-bit for memory efficiency (fits on 24GB GPU)
load_in_8bit: false
load_in_4bit: true

# LoRA adapter config (efficient fine-tuning)
adapter: qlora
lora_model_dir:
lora_r: 64
lora_alpha: 32
lora_dropout: 0.05
lora_target_linear: true
lora_fan_in_fan_out:

# Dataset
datasets:
  - path: ./dataset/sred_training_data.jsonl
    type: sharegpt
    conversation: chatml

# Output
output_dir: ./output/sred-mistral-7b-qlora

# Sequence length
sequence_len: 4096
sample_packing: true
pad_to_sequence_len: true

# Wandb logging (optional - set WANDB_API_KEY env var)
wandb_project: sred-report-writer
wandb_entity:
wandb_watch:
wandb_name: sred-mistral-7b-run1
wandb_log_model:

# Training hyperparameters
gradient_accumulation_steps: 4
micro_batch_size: 2
num_epochs: 3
optimizer: paged_adamw_32bit
lr_scheduler: cosine
learning_rate: 0.0002
weight_decay: 0.01
max_grad_norm: 1.0

# Warmup
warmup_steps: 20
warmup_ratio: 0.05

# Training settings
train_on_inputs: false
group_by_length: false
bf16: auto
fp16:
tf32: false

# Gradient checkpointing (saves memory)
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

# Logging
logging_steps: 5
save_strategy: steps
save_steps: 50
save_total_limit: 3
eval_steps: 50

# Flash attention (faster training on supported GPUs)
flash_attention: true

# DeepSpeed (uncomment for multi-GPU)
# deepspeed: deepspeed_configs/zero2.json

# Misc
early_stopping_patience: 3
auto_resume_from_checkpoints: true
local_rank:
special_tokens:
  pad_token: "<pad>"
